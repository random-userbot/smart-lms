# üöÄ GPU Setup Guide for LSTM Training - NVIDIA GeForce GTX 1650

## üîç Current Status

### ‚úÖ Hardware Detected:
- **GPU**: NVIDIA GeForce GTX 1650
- **Driver Version**: 31.0.15.2904
- **Intel GPU**: Intel Iris Xe Graphics (integrated)

### ‚ùå Software Issues:
- **nvidia-smi**: Not accessible in PATH
- **TensorFlow**: Cannot detect GPU (missing CUDA/cuDNN)
- **PyTorch**: CPU-only version installed (2.1.2+cpu)

---

## üìã Solution: Fix GPU Access for Deep Learning

### **Option 1: Quick Fix - Use TensorFlow with DirectML (Recommended for Windows)**

DirectML allows TensorFlow to use your GPU without CUDA installation.

#### Step 1: Install TensorFlow-DirectML
```powershell
# Uninstall current TensorFlow
pip uninstall tensorflow tensorflow-gpu

# Install TensorFlow with DirectML (supports NVIDIA, AMD, Intel GPUs)
pip install tensorflow-directml-plugin
```

#### Step 2: Verify GPU Detection
```python
import tensorflow as tf
print("TensorFlow version:", tf.__version__)
print("GPUs available:", tf.config.list_physical_devices('GPU'))

# Test GPU computation
with tf.device('/GPU:0'):
    a = tf.constant([[1.0, 2.0], [3.0, 4.0]])
    b = tf.constant([[1.0, 1.0], [0.0, 1.0]])
    c = tf.matmul(a, b)
    print("GPU computation successful:", c.numpy())
```

#### Step 3: Train LSTM on GPU
```python
# Your LSTM training code will automatically use GPU
model = keras.Sequential([
    LSTM(128, return_sequences=True, input_shape=(30, 22)),
    Dropout(0.3),
    LSTM(64),
    Dropout(0.3),
    Dense(32, activation='relu'),
    Dense(4, activation='softmax')
])

# No code changes needed - DirectML handles GPU automatically
model.fit(X_train, y_train, epochs=100, batch_size=32)
```

---

### **Option 2: Full CUDA Setup (More Complex, Better Performance)**

If you want maximum performance and access to `nvidia-smi`, install CUDA Toolkit.

#### Step 1: Check Compatible CUDA Version
Your GTX 1650 supports **CUDA 11.x or 12.x**.

For **TensorFlow 2.15.1** (your current version):
- Requires: **CUDA 11.8** + **cuDNN 8.6**

#### Step 2: Download NVIDIA Components

##### A. CUDA Toolkit 11.8
1. Visit: https://developer.nvidia.com/cuda-11-8-0-download-archive
2. Select:
   - Operating System: Windows
   - Architecture: x86_64
   - Version: 11
   - Installer Type: exe (network)
3. Download and run installer (~3 GB)
4. Install to default location: `C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.8`

##### B. cuDNN 8.6 for CUDA 11.8
1. Visit: https://developer.nvidia.com/cudnn-downloads
2. Create free NVIDIA Developer account
3. Download: **cuDNN v8.6.0 for CUDA 11.8**
4. Extract ZIP file
5. Copy files:
   ```
   cudnn-windows-x86_64-8.6.x.x\bin\*.dll       ‚Üí C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.8\bin\
   cudnn-windows-x86_64-8.6.x.x\include\*.h     ‚Üí C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.8\include\
   cudnn-windows-x86_64-8.6.x.x\lib\*.lib       ‚Üí C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.8\lib\x64\
   ```

#### Step 3: Update Environment Variables
```powershell
# Add to System PATH (requires admin rights)
# Run PowerShell as Administrator:

[Environment]::SetEnvironmentVariable(
    "Path",
    [Environment]::GetEnvironmentVariable("Path", "Machine") + 
    ";C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.8\bin;" +
    ";C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.8\libnvvp;",
    "Machine"
)

# Add CUDA_PATH
[Environment]::SetEnvironmentVariable(
    "CUDA_PATH",
    "C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.8",
    "Machine"
)

# Restart PowerShell after this
```

#### Step 4: Verify nvidia-smi
```powershell
# Close and reopen PowerShell, then:
nvidia-smi

# Expected output:
# +-----------------------------------------------------------------------------+
# | NVIDIA-SMI 531.xx       Driver Version: 531.xx       CUDA Version: 12.x  |
# |-------------------------------+----------------------+----------------------+
# | GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |
# | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
# |===============================+======================+======================|
# |   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0 Off |                  N/A |
# | N/A   45C    P0    N/A /  N/A |    123MiB /  4096MiB |      0%      Default |
# +-------------------------------+----------------------+----------------------+
```

#### Step 5: Install TensorFlow-GPU
```powershell
# Uninstall CPU version
pip uninstall tensorflow

# Install GPU version
pip install tensorflow==2.15.1

# Verify
python -c "import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))"
# Expected: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
```

---

### **Option 3: Use PyTorch with CUDA (Alternative to TensorFlow)**

If you prefer PyTorch for LSTM training:

#### Step 1: Install PyTorch with CUDA
```powershell
# Uninstall CPU version
pip uninstall torch torchvision torchaudio

# Install GPU version (CUDA 11.8)
pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

#### Step 2: Verify GPU
```python
import torch
print("PyTorch version:", torch.__version__)
print("CUDA available:", torch.cuda.is_available())
print("GPU device:", torch.cuda.get_device_name(0) if torch.cuda.is_available() else "N/A")
print("Current device:", torch.cuda.current_device() if torch.cuda.is_available() else "N/A")
```

#### Step 3: PyTorch LSTM Training Code
```python
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset

# Set device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

# Define LSTM model
class EngagementLSTM(nn.Module):
    def __init__(self, input_size=22, hidden_size=128, num_layers=2, num_classes=4):
        super(EngagementLSTM, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, 
                           batch_first=True, dropout=0.3)
        self.fc1 = nn.Linear(hidden_size, 32)
        self.fc2 = nn.Linear(32, num_classes)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(0.3)
    
    def forward(self, x):
        # x shape: (batch, seq_len=30, features=22)
        lstm_out, _ = self.lstm(x)
        last_hidden = lstm_out[:, -1, :]  # Take last time step
        x = self.relu(self.fc1(last_hidden))
        x = self.dropout(x)
        x = self.fc2(x)
        return x

# Initialize model and move to GPU
model = EngagementLSTM().to(device)

# Training loop
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# Move data to GPU
X_tensor = torch.FloatTensor(X_train).to(device)
y_tensor = torch.LongTensor(y_train).to(device)

dataset = TensorDataset(X_tensor, y_tensor)
dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

# Train
model.train()
for epoch in range(100):
    for batch_X, batch_y in dataloader:
        # Data already on GPU
        outputs = model(batch_X)
        loss = criterion(outputs, batch_y)
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    
    print(f"Epoch {epoch+1}/100, Loss: {loss.item():.4f}")

# Save model
torch.save(model.state_dict(), 'models/engagement_lstm_pytorch.pth')
```

---

## ‚ö° Recommended Approach for You

### **Choose Option 1: TensorFlow-DirectML** ‚úÖ

**Why?**
- ‚úÖ **Easiest**: No CUDA installation needed
- ‚úÖ **Fast setup**: 1 command (`pip install tensorflow-directml-plugin`)
- ‚úÖ **Works immediately**: Uses Windows DirectX for GPU acceleration
- ‚úÖ **Cross-GPU support**: Works with NVIDIA, AMD, Intel GPUs
- ‚ö†Ô∏è **Slight performance trade-off**: ~10-20% slower than native CUDA

**For Your Use Case (LSTM Training):**
- Training time with DirectML: ~15-20 minutes for 100 epochs
- Training time with CPU: ~2-3 hours
- **Speed up: ~8-10x faster than CPU**

---

## üöÄ Quick Start Commands

### Step 1: Install TensorFlow-DirectML
```powershell
cd "c:\Users\revan\Downloads\multiple lectures\multiple lectures"
pip uninstall tensorflow tensorflow-gpu -y
pip install tensorflow-directml-plugin
```

### Step 2: Test GPU
```powershell
python -c "import tensorflow as tf; print('GPUs:', tf.config.list_physical_devices('GPU'))"
```

### Step 3: Update Your Training Script
Add at the top of your LSTM training script:
```python
import tensorflow as tf

# Check GPU
gpus = tf.config.list_physical_devices('GPU')
if gpus:
    print(f"‚úÖ Using GPU: {gpus[0].name}")
    # Set memory growth to avoid OOM errors
    for gpu in gpus:
        tf.config.experimental.set_memory_growth(gpu, True)
else:
    print("‚ö†Ô∏è No GPU detected, using CPU")

# Rest of your training code...
```

---

## üîß Troubleshooting

### Issue 1: "Could not load dynamic library 'cudart64_110.dll'"
**Solution**: This means TensorFlow is looking for CUDA. Either:
- Install CUDA 11.x (Option 2), OR
- Use TensorFlow-DirectML (Option 1)

### Issue 2: GPU memory errors (OOM)
**Solution**: Reduce batch size
```python
# Instead of batch_size=32
model.fit(X_train, y_train, batch_size=16, epochs=100)
```

### Issue 3: nvidia-smi still not working after CUDA install
**Solution**: Check PATH
```powershell
# Run as Administrator
$env:Path += ";C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.8\bin"
nvidia-smi
```

### Issue 4: DirectML not detecting GPU
**Solution**: Update Windows and GPU drivers
```powershell
# Check Windows version (needs Windows 10 version 1903+)
winver

# Update GPU drivers from NVIDIA website:
# https://www.nvidia.com/Download/index.aspx
# Select: GeForce GTX 1650, Windows 11/10
```

---

## üìä Performance Comparison

| Method | Setup Time | Training Speed (100 epochs) | Compatibility |
|--------|-----------|----------------------------|---------------|
| **CPU Only** | 0 min | ~2-3 hours | ‚úÖ All systems |
| **DirectML** | 5 min | ~15-20 min | ‚úÖ Windows 10+ |
| **CUDA Native** | 30-60 min | ~10-15 min | ‚úÖ NVIDIA only |
| **PyTorch CUDA** | 30-60 min | ~10-15 min | ‚úÖ NVIDIA only |

---

## üéØ Next Steps

1. **Install TensorFlow-DirectML** (5 minutes)
   ```powershell
   pip install tensorflow-directml-plugin
   ```

2. **Verify GPU detection** (1 minute)
   ```powershell
   python -c "import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))"
   ```

3. **Run LSTM training** (15-20 minutes)
   ```powershell
   cd daisee-lstm-training
   python scripts/03_train_lstm.py
   ```

4. **Monitor GPU usage during training**
   - Open Task Manager ‚Üí Performance ‚Üí GPU 0
   - Should see ~80-100% GPU utilization during training

---

## üìö Additional Resources

- **TensorFlow GPU Guide**: https://www.tensorflow.org/install/gpu
- **DirectML Plugin**: https://github.com/microsoft/tensorflow-directml-plugin
- **PyTorch CUDA**: https://pytorch.org/get-started/locally/
- **NVIDIA CUDA Toolkit**: https://developer.nvidia.com/cuda-downloads
- **cuDNN Library**: https://developer.nvidia.com/cudnn

---

## ‚úÖ Summary

**Your GTX 1650 is ready for LSTM training!**

**Fastest Solution:**
```powershell
# 1. Install DirectML
pip uninstall tensorflow -y
pip install tensorflow-directml-plugin

# 2. Test
python -c "import tensorflow as tf; print('GPU Ready:', len(tf.config.list_physical_devices('GPU')) > 0)"

# 3. Train
python scripts/03_train_lstm.py
```

**Expected Output:**
- GPU detection: ‚úÖ
- Training speed: **8-10x faster** than CPU
- Training time: ~15-20 minutes (vs 2-3 hours on CPU)

üöÄ **You're all set! Your GPU will significantly speed up LSTM training.**
